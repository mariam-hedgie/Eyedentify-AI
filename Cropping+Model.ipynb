{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2q27gKz1H20"
   },
   "source": [
    "##### Copyright 2023 The MediaPipe Authors. All Rights Reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "TUfAcER1oUS6"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_cQX8dWu4Dv"
   },
   "source": [
    "# Face Landmarks Detection with MediaPipe Tasks\n",
    "\n",
    "This notebook shows you how to use MediaPipe Tasks Python API to detect face landmarks from images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6PN9FvIx614"
   },
   "source": [
    "## Preparation\n",
    "\n",
    "Let's start with installing MediaPipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gxbHBsF-8Y_l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a49D7h4TVmru"
   },
   "source": [
    "Then download the off-the-shelf model bundle(s). Check out the [MediaPipe documentation](https://developers.google.com/mediapipe/solutions/vision/face_landmarker#models) for more information about these model bundles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OMjuVQiDYJKF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n"
     ]
    }
   ],
   "source": [
    "!wget -O face_landmarker_v2_with_blendshapes.task -q https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYKAJ5nDU8-I"
   },
   "source": [
    "## Visualization utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "s3E6NFV-00Qt"
   },
   "outputs": [],
   "source": [
    "#@markdown We implemented some functions to visualize the face landmark detection results. <br/> Run the following cell to activate the functions.\n",
    "\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "  face_landmarks_list = detection_result.face_landmarks\n",
    "  annotated_image = np.copy(rgb_image)\n",
    "\n",
    "  # Loop through the detected faces to visualize.\n",
    "  for idx in range(len(face_landmarks_list)):\n",
    "    face_landmarks = face_landmarks_list[idx]\n",
    "\n",
    "    # Draw the face landmarks.\n",
    "    face_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    face_landmarks_proto.landmark.extend([\n",
    "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in face_landmarks\n",
    "    ])\n",
    "\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks_proto,\n",
    "        connections=mp.solutions.face_mesh.FACEMESH_TESSELATION,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp.solutions.drawing_styles\n",
    "        .get_default_face_mesh_tesselation_style())\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks_proto,\n",
    "        connections=mp.solutions.face_mesh.FACEMESH_CONTOURS,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp.solutions.drawing_styles\n",
    "        .get_default_face_mesh_contours_style())\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks_proto,\n",
    "        connections=mp.solutions.face_mesh.FACEMESH_IRISES,\n",
    "          landmark_drawing_spec=None,\n",
    "          connection_drawing_spec=mp.solutions.drawing_styles\n",
    "          .get_default_face_mesh_iris_connections_style())\n",
    "\n",
    "  return annotated_image\n",
    "\n",
    "def plot_face_blendshapes_bar_graph(face_blendshapes):\n",
    "  # Extract the face blendshapes category names and scores.\n",
    "  face_blendshapes_names = [face_blendshapes_category.category_name for face_blendshapes_category in face_blendshapes]\n",
    "  face_blendshapes_scores = [face_blendshapes_category.score for face_blendshapes_category in face_blendshapes]\n",
    "  # The blendshapes are ordered in decreasing score value.\n",
    "  face_blendshapes_ranks = range(len(face_blendshapes_names))\n",
    "\n",
    "  fig, ax = plt.subplots(figsize=(12, 12))\n",
    "  bar = ax.barh(face_blendshapes_ranks, face_blendshapes_scores, label=[str(x) for x in face_blendshapes_ranks])\n",
    "  ax.set_yticks(face_blendshapes_ranks, face_blendshapes_names)\n",
    "  ax.invert_yaxis()\n",
    "\n",
    "  # Label each bar with values\n",
    "  for score, patch in zip(face_blendshapes_scores, bar.patches):\n",
    "    plt.text(patch.get_x() + patch.get_width(), patch.get_y(), f\"{score:.4f}\", va=\"top\")\n",
    "\n",
    "  ax.set_xlabel('Score')\n",
    "  ax.set_title(\"Face Blendshapes\")\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83PEJNp9yPBU"
   },
   "source": [
    "## Download test image\n",
    "\n",
    "Let's grab a test image that we'll use later. The image is from [Unsplash](https://unsplash.com/photos/mt2fyrdXxzk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tzXuqyIBlXer"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m get_ipython().system(\u001b[33m'\u001b[39m\u001b[33mwget -q -O image.png https://storage.googleapis.com/mediapipe-assets/business-person.png\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpatches\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cv2_imshow\n\u001b[32m      6\u001b[39m img = cv2.imread(\u001b[33m\"\u001b[39m\u001b[33mimage.png\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m cv2_imshow(img)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "!wget -q -O image.png https://storage.googleapis.com/mediapipe-assets/business-person.png\n",
    "\n",
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "img = cv2.imread(\"image.png\")\n",
    "cv2_imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-skLwMBmMN_"
   },
   "source": [
    "Optionally, you can upload your own image. If you want to do so, uncomment and run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "etBjSdwImQPw"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "\n",
    "# for filename in uploaded:\n",
    "#   content = uploaded[filename]\n",
    "#   with open(filename, 'wb') as f:\n",
    "#     f.write(content)\n",
    "\n",
    "# if len(uploaded.keys()):\n",
    "#   IMAGE_FILE = next(iter(uploaded))\n",
    "#   print('Uploaded file:', IMAGE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@145.446] global loadsave.cpp:268 findDecoder imread_('Eye Detector V2/test/1-6838-_jpg.rf.4c5b68134ef020d3ab4ce0d59a80a22f.jpg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31merror\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Load and display\u001b[39;00m\n\u001b[32m      7\u001b[39m img = cv2.imread(IMAGE_FILE)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m img_rgb = \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m plt.imshow(img_rgb)\n\u001b[32m     11\u001b[39m plt.axis(\u001b[33m'\u001b[39m\u001b[33moff\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31merror\u001b[39m: OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "IMAGE_FILE = \"Eye Detector V2/test/1-6838-_jpg.rf.4c5b68134ef020d3ab4ce0d59a80a22f.jpg\"\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and display\n",
    "img = cv2.imread(IMAGE_FILE)\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(img_rgb)\n",
    "plt.axis('off')\n",
    "plt.title(\"Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iy4r2_ePylIa"
   },
   "source": [
    "## Running inference and visualizing the results\n",
    "\n",
    "Here are the steps to run face landmark detection using MediaPipe.\n",
    "\n",
    "Check out the [MediaPipe documentation](https://developers.google.com/mediapipe/solutions/vision/face_landmarker/python) to learn more about configuration options that this task supports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_JVO3rvPD4RN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1752081939.401770 32613049 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Max\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unable to open file at /Users/mariamhusain/Desktop/eyedentify-ai/face_landmarker.task",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      7\u001b[39m base_options = python.BaseOptions(model_asset_path=\u001b[33m'\u001b[39m\u001b[33mface_landmarker.task\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m options = vision.FaceLandmarkerOptions(base_options=base_options,\n\u001b[32m      9\u001b[39m                                        output_face_blendshapes=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     10\u001b[39m                                        output_facial_transformation_matrixes=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     11\u001b[39m                                        num_faces=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m detector = \u001b[43mvision\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFaceLandmarker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_from_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# STEP 3: Load the input image.\u001b[39;00m\n\u001b[32m     15\u001b[39m image = mp.Image.create_from_file(IMAGE_FILE)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/eyedentify-ai/.venv/lib/python3.12/site-packages/mediapipe/tasks/python/vision/face_landmarker.py:3104\u001b[39m, in \u001b[36mFaceLandmarker.create_from_options\u001b[39m\u001b[34m(cls, options)\u001b[39m\n\u001b[32m   3091\u001b[39m   output_streams.append(\n\u001b[32m   3092\u001b[39m       \u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[33m'\u001b[39m.join([_FACE_GEOMETRY_TAG, _FACE_GEOMETRY_STREAM_NAME])\n\u001b[32m   3093\u001b[39m   )\n\u001b[32m   3095\u001b[39m task_info = _TaskInfo(\n\u001b[32m   3096\u001b[39m     task_graph=_TASK_GRAPH_NAME,\n\u001b[32m   3097\u001b[39m     input_streams=[\n\u001b[32m   (...)\u001b[39m\u001b[32m   3102\u001b[39m     task_options=options,\n\u001b[32m   3103\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   3105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_graph_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3106\u001b[39m \u001b[43m        \u001b[49m\u001b[43menable_flow_limiting\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrunning_mode\u001b[49m\n\u001b[32m   3107\u001b[39m \u001b[43m        \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43m_RunningMode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLIVE_STREAM\u001b[49m\n\u001b[32m   3108\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3109\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrunning_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3110\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpackets_callback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult_callback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   3111\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/eyedentify-ai/.venv/lib/python3.12/site-packages/mediapipe/tasks/python/vision/core/base_vision_task_api.py:70\u001b[39m, in \u001b[36mBaseVisionTaskApi.__init__\u001b[39m\u001b[34m(self, graph_config, running_mode, packet_callback)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m packet_callback:\n\u001b[32m     66\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     67\u001b[39m       \u001b[33m'\u001b[39m\u001b[33mThe vision task is in image or video mode, a user-defined result \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     68\u001b[39m       \u001b[33m'\u001b[39m\u001b[33mcallback should not be provided.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     69\u001b[39m   )\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m \u001b[38;5;28mself\u001b[39m._runner = \u001b[43m_TaskRunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpacket_callback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[38;5;28mself\u001b[39m._running_mode = running_mode\n",
      "\u001b[31mRuntimeError\u001b[39m: Unable to open file at /Users/mariamhusain/Desktop/eyedentify-ai/face_landmarker.task"
     ]
    }
   ],
   "source": [
    "# STEP 1: Import the necessary modules.\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# STEP 2: Create an FaceLandmarker object.\n",
    "base_options = python.BaseOptions(model_asset_path='face_landmarker.task')\n",
    "options = vision.FaceLandmarkerOptions(base_options=base_options,\n",
    "                                       output_face_blendshapes=True,\n",
    "                                       output_facial_transformation_matrixes=True,\n",
    "                                       num_faces=1)\n",
    "detector = vision.FaceLandmarker.create_from_options(options)\n",
    "\n",
    "# STEP 3: Load the input image.\n",
    "image = mp.Image.create_from_file(IMAGE_FILE)\n",
    "\n",
    "# STEP 4: Detect face landmarks from the input image.\n",
    "detection_result = detector.detect(image)\n",
    "\n",
    "import cv2\n",
    "rgb_image = image.numpy_view()\n",
    "bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "annotated_image = draw_landmarks_on_image(bgr_image, detection_result)\n",
    "\n",
    "# STEP 5: Process the detection result. In this case, visualize it.\n",
    "# annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB))  # convert back for display\n",
    "plt.axis('off')\n",
    "plt.title(\"Face Landmarks\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKelLdIfwL4V"
   },
   "source": [
    "We will also visualize the face blendshapes categories using a bar graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "l0id2t5Vl83m"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'detection_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m plot_face_blendshapes_bar_graph(\u001b[43mdetection_result\u001b[49m.face_blendshapes[\u001b[32m0\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'detection_result' is not defined"
     ]
    }
   ],
   "source": [
    "plot_face_blendshapes_bar_graph(detection_result.face_blendshapes[0]) # we don't know"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckKurV96cG01"
   },
   "source": [
    "And print the transformation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xixKF10-rmse"
   },
   "outputs": [],
   "source": [
    "print(detection_result.facial_transformation_matrixes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      6\u001b[39m pad = \u001b[32m10\u001b[39m\n\u001b[32m      9\u001b[39m image_folder = \u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m image_paths = [os.path.join(image_folder, f)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m                \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m                \u001b[38;5;28;01mif\u001b[39;00m f.lower().endswith((\u001b[33m\"\u001b[39m\u001b[33m.jpg\u001b[39m\u001b[33m\"\u001b[39m))]\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(image_paths):\n\u001b[32m     15\u001b[39m     mp_image = mp.Image.create_from_file(path)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'test'"
     ]
    }
   ],
   "source": [
    "# change path to relative\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "left_eye_ids = [33, 133, 159, 160, 161, 144, 145, 153]\n",
    "right_eye_ids = [362, 263, 386, 387, 388, 373, 374, 380]\n",
    "pad = 10\n",
    "\n",
    "\n",
    "image_folder = \"test\"\n",
    "image_paths = [os.path.join(image_folder, f)\n",
    "               for f in os.listdir(image_folder)\n",
    "               if f.lower().endswith((\".jpg\"))]\n",
    "\n",
    "for i, path in enumerate(image_paths):\n",
    "    mp_image = mp.Image.create_from_file(path)\n",
    "    detection_result = detector.detect(mp_image)\n",
    "\n",
    "    if not detection_result.face_landmarks:\n",
    "        print(f\"❌ No face detected in {path}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Pick one detected face\n",
    "    face_landmarks = detection_result.face_landmarks[0]\n",
    "    \n",
    "    rgb_image = mp_image.numpy_view()\n",
    "    h, w, _ = rgb_image.shape\n",
    "    \n",
    "    xs = [int(face_landmarks[i].x * w) for i in left_eye_ids]\n",
    "    ys = [int(face_landmarks[i].y * h) for i in left_eye_ids]\n",
    "\n",
    "    xmin, xmax = max(min(xs) - pad, 0), min(max(xs) + pad, w)\n",
    "    ymin, ymax = max(min(ys) - pad, 0), min(max(ys) + pad, h)\n",
    "    \n",
    "    # Crop the eye region from the original RGB image\n",
    "    eye_crop = rgb_image[ymin:ymax, xmin:xmax]\n",
    "\n",
    "    out_path = os.path.join(\"cropped_images/left\", f\"eye_{i}.png\")\n",
    "    cv2.imwrite(out_path, eye_crop)\n",
    "    \n",
    "    # Get right eye corners \n",
    "    xs = [int(face_landmarks[i].x * w) for i in right_eye_ids]\n",
    "    ys = [int(face_landmarks[i].y * h) for i in right_eye_ids]\n",
    "\n",
    "    xmin, xmax = max(min(xs) - pad, 0), min(max(xs) + pad, w)\n",
    "    ymin, ymax = max(min(ys) - pad, 0), min(max(ys) + pad, h)\n",
    "    \n",
    "    # Crop the eye region from the original RGB image\n",
    "    eye_crop = rgb_image[ymin:ymax, xmin:xmax]\n",
    "    \n",
    "    out_path = os.path.join(\"cropped_images/right\", f\"eye_{i}.png\")\n",
    "    cv2.imwrite(out_path, eye_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Eyedentify-AI/data/filtered/infected_eye'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      6\u001b[39m pad = \u001b[32m10\u001b[39m\n\u001b[32m      9\u001b[39m image_folder = \u001b[33m\"\u001b[39m\u001b[33mEyedentify-AI/data/filtered/infected_eye\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m image_paths = [os.path.join(image_folder, f)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m                \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m                \u001b[38;5;28;01mif\u001b[39;00m f.lower().endswith((\u001b[33m\"\u001b[39m\u001b[33m.jpg\u001b[39m\u001b[33m\"\u001b[39m))]\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(image_paths):\n\u001b[32m     15\u001b[39m     mp_image = mp.Image.create_from_file(path)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'Eyedentify-AI/data/filtered/infected_eye'"
     ]
    }
   ],
   "source": [
    "# for cropped eyes - won't work\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "left_eye_ids = [33, 133, 159, 160, 161, 144, 145, 153]\n",
    "right_eye_ids = [362, 263, 386, 387, 388, 373, 374, 380]\n",
    "pad = 10\n",
    "\n",
    "\n",
    "image_folder = \"Eyedentify-AI/data/filtered/infected_eye\"\n",
    "image_paths = [os.path.join(image_folder, f)\n",
    "               for f in os.listdir(image_folder)\n",
    "               if f.lower().endswith((\".jpg\"))]\n",
    "\n",
    "for i, path in enumerate(image_paths):\n",
    "    mp_image = mp.Image.create_from_file(path)\n",
    "    detection_result = detector.detect(mp_image)\n",
    "\n",
    "    if not detection_result.face_landmarks:\n",
    "        print(f\"❌ No face detected in {path}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Pick one detected face\n",
    "    face_landmarks = detection_result.face_landmarks[0]\n",
    "    \n",
    "    rgb_image = mp_image.numpy_view()\n",
    "    h, w, _ = rgb_image.shape\n",
    "    \n",
    "    xs = [int(face_landmarks[i].x * w) for i in left_eye_ids]\n",
    "    ys = [int(face_landmarks[i].y * h) for i in left_eye_ids]\n",
    "\n",
    "    xmin, xmax = max(min(xs) - pad, 0), min(max(xs) + pad, w)\n",
    "    ymin, ymax = max(min(ys) - pad, 0), min(max(ys) + pad, h)\n",
    "    \n",
    "    # Crop the eye region from the original RGB image\n",
    "    eye_crop = rgb_image[ymin:ymax, xmin:xmax]\n",
    "\n",
    "    out_path = os.path.join(\"cropped_images_infected/left\", f\"eye_{i}.png\")\n",
    "    cv2.imwrite(out_path, eye_crop)\n",
    "    \n",
    "    # Get right eye corners \n",
    "    xs = [int(face_landmarks[i].x * w) for i in right_eye_ids]\n",
    "    ys = [int(face_landmarks[i].y * h) for i in right_eye_ids]\n",
    "\n",
    "    xmin, xmax = max(min(xs) - pad, 0), min(max(xs) + pad, w)\n",
    "    ymin, ymax = max(min(ys) - pad, 0), min(max(ys) + pad, h)\n",
    "    \n",
    "    # Crop the eye region from the original RGB image\n",
    "    eye_crop = rgb_image[ymin:ymax, xmin:xmax]\n",
    "    \n",
    "    out_path = os.path.join(\"cropped_images_infected/right\", f\"eye_{i}.png\")\n",
    "    cv2.imwrite(out_path, eye_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=2.4.0\n",
      "=2.6.0\n",
      "\u001b[34mAppData\u001b[m\u001b[m\n",
      "\u001b[34mApplications\u001b[m\u001b[m\n",
      "\u001b[34mApplications (Parallels)\u001b[m\u001b[m\n",
      "\u001b[34mbaltimorefoodline\u001b[m\u001b[m\n",
      "\u001b[34mcar-counting-and-speed-estimation-yolo-sort-python\u001b[m\u001b[m\n",
      "cookies.txt\n",
      "\u001b[34mcropped_images\u001b[m\u001b[m\n",
      "\u001b[34mcropped_images_infected\u001b[m\u001b[m\n",
      "\u001b[34mDesktop\u001b[m\u001b[m\n",
      "\u001b[34mdist\u001b[m\u001b[m\n",
      "\u001b[34mDocuments\u001b[m\u001b[m\n",
      "\u001b[34mDownloads\u001b[m\u001b[m\n",
      "eeg-mi.ipynb\n",
      "ET7 Op.mp4\n",
      "ET7-framets.npy\n",
      "\u001b[34mExecutiveCoachApp\u001b[m\u001b[m\n",
      "\u001b[34mEyedentify-AI\u001b[m\u001b[m\n",
      "\u001b[34mFLARe\u001b[m\u001b[m\n",
      "\u001b[34mflare-app\u001b[m\u001b[m\n",
      "\u001b[34mflare-portal\u001b[m\u001b[m\n",
      "\u001b[34mflare2\u001b[m\u001b[m\n",
      "\u001b[34mflask-birthday-reminder\u001b[m\u001b[m\n",
      "\u001b[34mGazeTracking\u001b[m\u001b[m\n",
      "get-pip.py\n",
      "\u001b[34mgit_flare_portal\u001b[m\u001b[m\n",
      "\u001b[34mide-portfolio\u001b[m\u001b[m\n",
      "\u001b[34mIdeaProjects\u001b[m\u001b[m\n",
      "\u001b[34mIdeaSnapshots\u001b[m\u001b[m\n",
      "\u001b[34mLibrary\u001b[m\u001b[m\n",
      "\u001b[34mlucky-portfolio\u001b[m\u001b[m\n",
      "main.spec\n",
      "\u001b[34mMARBLE\u001b[m\u001b[m\n",
      "marble_X.npy\n",
      "MARBLE.ipynb\n",
      "\u001b[34mMovies\u001b[m\u001b[m\n",
      "\u001b[34mMusic\u001b[m\u001b[m\n",
      "\u001b[34mnextjs-ai-chatbot\u001b[m\u001b[m\n",
      "\u001b[34mnltk_data\u001b[m\u001b[m\n",
      "\u001b[34mnode_modules\u001b[m\u001b[m\n",
      "\u001b[34mosd\u001b[m\u001b[m\n",
      "package-lock.json\n",
      "package.json\n",
      "\u001b[34mPictures\u001b[m\u001b[m\n",
      "\u001b[34mpikaraoke\u001b[m\u001b[m\n",
      "pnpm-lock.yaml\n",
      "\u001b[34mportfolio\u001b[m\u001b[m\n",
      "\u001b[34mPublic\u001b[m\u001b[m\n",
      "pupil_class_labels.npy\n",
      "\u001b[34mpyEnvs\u001b[m\u001b[m\n",
      "README\n",
      "\u001b[34msemesterly\u001b[m\u001b[m\n",
      "server.js\n",
      "\u001b[34mSIS-Bot\u001b[m\u001b[m\n",
      "\u001b[34mspeed-detector\u001b[m\u001b[m\n",
      "\u001b[34mtest\u001b[m\u001b[m\n",
      "test.py\n",
      "\u001b[34mtextbelt\u001b[m\u001b[m\n",
      "TryOne.py\n",
      "Untitled.ipynb\n",
      "Untitled1.ipynb\n",
      "Untitled2.ipynb\n",
      "\u001b[34mUsers\u001b[m\u001b[m\n",
      "\u001b[34mvs-code-style-site\u001b[m\u001b[m\n",
      "\u001b[34mWin\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shiningdiamond\n"
     ]
    }
   ],
   "source": [
    "cd ./../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 1295\n",
      "Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Labels saved to image_labels.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# Define the paths to your two image folders\n",
    "folder_1 = 'Eyedentify-AI/data/filtered/healthy_eye'  # Folder for class 0\n",
    "folder_2 = 'Eyedentify-AI/data/filtered/infected_eye'  # Folder for class 0\n",
    "\n",
    "# Generate the image paths and corresponding labels\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "# Add images from folder 1 (class 0)\n",
    "for img_name in os.listdir(folder_1):\n",
    "    if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):  # Adjust for your image types\n",
    "        image_paths.append(os.path.join(folder_1, img_name))\n",
    "        labels.append(0)\n",
    "\n",
    "# Add images from folder 2 (class 1)\n",
    "for img_name in os.listdir(folder_2):\n",
    "    if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):  # Adjust for your image types\n",
    "        image_paths.append(os.path.join(folder_2, img_name))\n",
    "        labels.append(1)\n",
    "\n",
    "# Check if the labels and paths are generated correctly\n",
    "print(f\"Total images: {len(image_paths)}\")\n",
    "print(f\"Labels: {labels[:10]}\")  # Print first 10 labels for confirmation\n",
    "\n",
    "# Define the file path for saving labels\n",
    "labels_file = 'image_labels.csv'\n",
    "\n",
    "# Save image paths and labels to a CSV file\n",
    "with open(labels_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['image_path', 'label'])  # Header row\n",
    "    for path, label in zip(image_paths, labels):\n",
    "        writer.writerow([path, label])\n",
    "\n",
    "print(f\"Labels saved to {labels_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 1295\n",
      "First 10 image paths: ['Eyedentify-AI/data/filtered/healthy_eye/63.jpg', 'Eyedentify-AI/data/filtered/healthy_eye/823.jpg', 'Eyedentify-AI/data/filtered/healthy_eye/189.jpg', 'Eyedentify-AI/data/filtered/healthy_eye/77.jpg', 'Eyedentify-AI/data/filtered/healthy_eye/638.jpg', 'Eyedentify-AI/data/filtered/healthy_eye/604.jpg', 'Eyedentify-AI/data/filtered/healthy_eye/162.jpg', 'Eyedentify-AI/data/filtered/healthy_eye/176.jpg', 'Eyedentify-AI/data/filtered/healthy_eye/88.jpg', 'Eyedentify-AI/data/filtered/healthy_eye/610.jpg']\n",
      "First 10 labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "--- Fold 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/cebra-env/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/cebra-env/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Val Loss = 0.6132, AUC = 0.7697, Accuracy = 0.6564\n",
      "Epoch 2: Val Loss = 0.5711, AUC = 0.8630, Accuracy = 0.7027\n",
      "Epoch 3: Val Loss = 0.5249, AUC = 0.9120, Accuracy = 0.7915\n",
      "Epoch 4: Val Loss = 0.4794, AUC = 0.9469, Accuracy = 0.8378\n",
      "Epoch 5: Val Loss = 0.4457, AUC = 0.9591, Accuracy = 0.8803\n",
      "Epoch 6: Val Loss = 0.4201, AUC = 0.9658, Accuracy = 0.8842\n",
      "Epoch 7: Val Loss = 0.4019, AUC = 0.9689, Accuracy = 0.8880\n",
      "Epoch 8: Val Loss = 0.3759, AUC = 0.9734, Accuracy = 0.9035\n",
      "Epoch 9: Val Loss = 0.3606, AUC = 0.9761, Accuracy = 0.8996\n",
      "Epoch 10: Val Loss = 0.3427, AUC = 0.9792, Accuracy = 0.9151\n",
      "Epoch 11: Val Loss = 0.3346, AUC = 0.9803, Accuracy = 0.9151\n",
      "Epoch 12: Val Loss = 0.3137, AUC = 0.9821, Accuracy = 0.9228\n",
      "Epoch 13: Val Loss = 0.3036, AUC = 0.9834, Accuracy = 0.9266\n",
      "Epoch 14: Val Loss = 0.2977, AUC = 0.9832, Accuracy = 0.9266\n",
      "Epoch 15: Val Loss = 0.2844, AUC = 0.9839, Accuracy = 0.9228\n",
      "Epoch 16: Val Loss = 0.2764, AUC = 0.9850, Accuracy = 0.9305\n",
      "Epoch 17: Val Loss = 0.2684, AUC = 0.9855, Accuracy = 0.9305\n",
      "Epoch 18: Val Loss = 0.2603, AUC = 0.9856, Accuracy = 0.9421\n",
      "Epoch 19: Val Loss = 0.2569, AUC = 0.9858, Accuracy = 0.9382\n",
      "Epoch 20: Val Loss = 0.2499, AUC = 0.9877, Accuracy = 0.9421\n",
      "Epoch 21: Val Loss = 0.2436, AUC = 0.9878, Accuracy = 0.9459\n",
      "Epoch 22: Val Loss = 0.2403, AUC = 0.9871, Accuracy = 0.9382\n",
      "Epoch 23: Val Loss = 0.2352, AUC = 0.9878, Accuracy = 0.9344\n",
      "Epoch 24: Val Loss = 0.2286, AUC = 0.9884, Accuracy = 0.9459\n",
      "Epoch 25: Val Loss = 0.2274, AUC = 0.9880, Accuracy = 0.9421\n",
      "Epoch 26: Val Loss = 0.2221, AUC = 0.9885, Accuracy = 0.9421\n",
      "Epoch 27: Val Loss = 0.2163, AUC = 0.9893, Accuracy = 0.9459\n",
      "Epoch 28: Val Loss = 0.2151, AUC = 0.9898, Accuracy = 0.9344\n",
      "Epoch 29: Val Loss = 0.2103, AUC = 0.9895, Accuracy = 0.9459\n",
      "Epoch 30: Val Loss = 0.2110, AUC = 0.9898, Accuracy = 0.9459\n",
      "\n",
      "--- Fold 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/cebra-env/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/cebra-env/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Val Loss = 0.6464, AUC = 0.7201, Accuracy = 0.6448\n",
      "Epoch 2: Val Loss = 0.5927, AUC = 0.8411, Accuracy = 0.7297\n",
      "Epoch 3: Val Loss = 0.5492, AUC = 0.8984, Accuracy = 0.8031\n",
      "Epoch 4: Val Loss = 0.5192, AUC = 0.9236, Accuracy = 0.8108\n",
      "Epoch 5: Val Loss = 0.4884, AUC = 0.9357, Accuracy = 0.8803\n",
      "Epoch 6: Val Loss = 0.4619, AUC = 0.9445, Accuracy = 0.8842\n",
      "Epoch 7: Val Loss = 0.4413, AUC = 0.9491, Accuracy = 0.8880\n",
      "Epoch 8: Val Loss = 0.4213, AUC = 0.9558, Accuracy = 0.8842\n",
      "Epoch 9: Val Loss = 0.4058, AUC = 0.9600, Accuracy = 0.8958\n",
      "Epoch 10: Val Loss = 0.3882, AUC = 0.9616, Accuracy = 0.8919\n",
      "Epoch 11: Val Loss = 0.3771, AUC = 0.9638, Accuracy = 0.8958\n",
      "Epoch 12: Val Loss = 0.3626, AUC = 0.9667, Accuracy = 0.8996\n",
      "Epoch 13: Val Loss = 0.3526, AUC = 0.9677, Accuracy = 0.8958\n",
      "Epoch 14: Val Loss = 0.3443, AUC = 0.9687, Accuracy = 0.8996\n",
      "Epoch 15: Val Loss = 0.3299, AUC = 0.9700, Accuracy = 0.8996\n",
      "Epoch 16: Val Loss = 0.3259, AUC = 0.9710, Accuracy = 0.8996\n",
      "Epoch 17: Val Loss = 0.3147, AUC = 0.9715, Accuracy = 0.9035\n",
      "Epoch 18: Val Loss = 0.3125, AUC = 0.9714, Accuracy = 0.8996\n",
      "Epoch 19: Val Loss = 0.3051, AUC = 0.9729, Accuracy = 0.9035\n",
      "Epoch 20: Val Loss = 0.2959, AUC = 0.9729, Accuracy = 0.9035\n",
      "Epoch 21: Val Loss = 0.2926, AUC = 0.9736, Accuracy = 0.8996\n",
      "Epoch 22: Val Loss = 0.2862, AUC = 0.9743, Accuracy = 0.9035\n",
      "Epoch 23: Val Loss = 0.2824, AUC = 0.9736, Accuracy = 0.8958\n",
      "Epoch 24: Val Loss = 0.2753, AUC = 0.9746, Accuracy = 0.8996\n",
      "Epoch 25: Val Loss = 0.2745, AUC = 0.9764, Accuracy = 0.9035\n",
      "Epoch 26: Val Loss = 0.2679, AUC = 0.9763, Accuracy = 0.9035\n",
      "Epoch 27: Val Loss = 0.2659, AUC = 0.9763, Accuracy = 0.8996\n",
      "Epoch 28: Val Loss = 0.2588, AUC = 0.9771, Accuracy = 0.9035\n",
      "Epoch 29: Val Loss = 0.2599, AUC = 0.9764, Accuracy = 0.8996\n",
      "Epoch 30: Val Loss = 0.2529, AUC = 0.9783, Accuracy = 0.9073\n",
      "\n",
      "--- Fold 3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/cebra-env/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/cebra-env/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Val Loss = 0.7102, AUC = 0.6120, Accuracy = 0.6293\n",
      "Epoch 2: Val Loss = 0.6389, AUC = 0.7800, Accuracy = 0.6834\n",
      "Epoch 3: Val Loss = 0.5877, AUC = 0.8727, Accuracy = 0.7645\n",
      "Epoch 4: Val Loss = 0.5455, AUC = 0.9120, Accuracy = 0.8031\n",
      "Epoch 5: Val Loss = 0.5104, AUC = 0.9387, Accuracy = 0.8378\n",
      "Epoch 6: Val Loss = 0.4743, AUC = 0.9541, Accuracy = 0.8803\n",
      "Epoch 7: Val Loss = 0.4462, AUC = 0.9642, Accuracy = 0.8958\n",
      "Epoch 8: Val Loss = 0.4196, AUC = 0.9690, Accuracy = 0.9112\n",
      "Epoch 9: Val Loss = 0.3979, AUC = 0.9726, Accuracy = 0.8996\n",
      "Epoch 10: Val Loss = 0.3764, AUC = 0.9753, Accuracy = 0.9035\n",
      "Epoch 11: Val Loss = 0.3639, AUC = 0.9782, Accuracy = 0.9266\n",
      "Epoch 12: Val Loss = 0.3438, AUC = 0.9789, Accuracy = 0.9189\n",
      "Epoch 13: Val Loss = 0.3284, AUC = 0.9796, Accuracy = 0.9189\n",
      "Epoch 14: Val Loss = 0.3198, AUC = 0.9830, Accuracy = 0.9382\n",
      "Epoch 15: Val Loss = 0.3087, AUC = 0.9847, Accuracy = 0.9459\n",
      "Epoch 16: Val Loss = 0.3017, AUC = 0.9846, Accuracy = 0.9228\n",
      "Epoch 17: Val Loss = 0.2926, AUC = 0.9859, Accuracy = 0.9498\n",
      "Epoch 18: Val Loss = 0.2843, AUC = 0.9857, Accuracy = 0.9459\n",
      "Epoch 19: Val Loss = 0.2734, AUC = 0.9867, Accuracy = 0.9459\n",
      "Epoch 20: Val Loss = 0.2683, AUC = 0.9870, Accuracy = 0.9498\n",
      "Epoch 21: Val Loss = 0.2619, AUC = 0.9872, Accuracy = 0.9498\n",
      "Epoch 22: Val Loss = 0.2552, AUC = 0.9872, Accuracy = 0.9498\n",
      "Epoch 23: Val Loss = 0.2537, AUC = 0.9874, Accuracy = 0.9537\n",
      "Epoch 24: Val Loss = 0.2418, AUC = 0.9877, Accuracy = 0.9498\n",
      "Epoch 25: Val Loss = 0.2351, AUC = 0.9882, Accuracy = 0.9498\n",
      "Epoch 26: Val Loss = 0.2347, AUC = 0.9888, Accuracy = 0.9498\n",
      "Epoch 27: Val Loss = 0.2309, AUC = 0.9887, Accuracy = 0.9498\n",
      "Epoch 28: Val Loss = 0.2257, AUC = 0.9890, Accuracy = 0.9459\n",
      "Epoch 29: Val Loss = 0.2212, AUC = 0.9901, Accuracy = 0.9498\n",
      "Epoch 30: Val Loss = 0.2184, AUC = 0.9895, Accuracy = 0.9498\n",
      "\n",
      "--- Fold 4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/cebra-env/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/cebra-env/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Val Loss = 0.8002, AUC = 0.2224, Accuracy = 0.3861\n",
      "Epoch 2: Val Loss = 0.7350, AUC = 0.3696, Accuracy = 0.4865\n",
      "Epoch 3: Val Loss = 0.6726, AUC = 0.5592, Accuracy = 0.5869\n",
      "Epoch 4: Val Loss = 0.6208, AUC = 0.7276, Accuracy = 0.6680\n",
      "Epoch 5: Val Loss = 0.5742, AUC = 0.8423, Accuracy = 0.7683\n",
      "Epoch 6: Val Loss = 0.5288, AUC = 0.9152, Accuracy = 0.8378\n",
      "Epoch 7: Val Loss = 0.4952, AUC = 0.9487, Accuracy = 0.8764\n",
      "Epoch 8: Val Loss = 0.4671, AUC = 0.9615, Accuracy = 0.9073\n",
      "Epoch 9: Val Loss = 0.4397, AUC = 0.9715, Accuracy = 0.9189\n",
      "Epoch 10: Val Loss = 0.4158, AUC = 0.9793, Accuracy = 0.9189\n",
      "Epoch 11: Val Loss = 0.3933, AUC = 0.9829, Accuracy = 0.9266\n",
      "Epoch 12: Val Loss = 0.3681, AUC = 0.9852, Accuracy = 0.9498\n",
      "Epoch 13: Val Loss = 0.3568, AUC = 0.9869, Accuracy = 0.9537\n",
      "Epoch 14: Val Loss = 0.3418, AUC = 0.9879, Accuracy = 0.9382\n",
      "Epoch 15: Val Loss = 0.3198, AUC = 0.9896, Accuracy = 0.9537\n",
      "Epoch 16: Val Loss = 0.3112, AUC = 0.9896, Accuracy = 0.9537\n",
      "Epoch 17: Val Loss = 0.2959, AUC = 0.9905, Accuracy = 0.9498\n",
      "Epoch 18: Val Loss = 0.2909, AUC = 0.9905, Accuracy = 0.9537\n",
      "Epoch 19: Val Loss = 0.2768, AUC = 0.9909, Accuracy = 0.9498\n",
      "Epoch 20: Val Loss = 0.2669, AUC = 0.9916, Accuracy = 0.9498\n",
      "Epoch 21: Val Loss = 0.2608, AUC = 0.9919, Accuracy = 0.9614\n",
      "Epoch 22: Val Loss = 0.2496, AUC = 0.9918, Accuracy = 0.9691\n",
      "Epoch 23: Val Loss = 0.2492, AUC = 0.9925, Accuracy = 0.9575\n",
      "Epoch 24: Val Loss = 0.2372, AUC = 0.9928, Accuracy = 0.9653\n",
      "Epoch 25: Val Loss = 0.2326, AUC = 0.9928, Accuracy = 0.9575\n",
      "Epoch 26: Val Loss = 0.2258, AUC = 0.9933, Accuracy = 0.9575\n",
      "Epoch 27: Val Loss = 0.2212, AUC = 0.9931, Accuracy = 0.9575\n",
      "Epoch 28: Val Loss = 0.2124, AUC = 0.9935, Accuracy = 0.9730\n",
      "Epoch 29: Val Loss = 0.2113, AUC = 0.9936, Accuracy = 0.9614\n",
      "Epoch 30: Val Loss = 0.2081, AUC = 0.9935, Accuracy = 0.9575\n",
      "\n",
      "--- Fold 5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/cebra-env/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/cebra-env/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Val Loss = 0.6687, AUC = 0.6418, Accuracy = 0.6332\n",
      "Epoch 2: Val Loss = 0.6178, AUC = 0.7891, Accuracy = 0.7143\n",
      "Epoch 3: Val Loss = 0.5661, AUC = 0.8840, Accuracy = 0.7799\n",
      "Epoch 4: Val Loss = 0.5258, AUC = 0.9276, Accuracy = 0.8340\n",
      "Epoch 5: Val Loss = 0.4919, AUC = 0.9511, Accuracy = 0.8726\n",
      "Epoch 6: Val Loss = 0.4638, AUC = 0.9634, Accuracy = 0.8764\n",
      "Epoch 7: Val Loss = 0.4394, AUC = 0.9675, Accuracy = 0.8842\n",
      "Epoch 8: Val Loss = 0.4183, AUC = 0.9707, Accuracy = 0.8919\n",
      "Epoch 9: Val Loss = 0.4025, AUC = 0.9713, Accuracy = 0.8842\n",
      "Epoch 10: Val Loss = 0.3817, AUC = 0.9744, Accuracy = 0.8880\n",
      "Epoch 11: Val Loss = 0.3652, AUC = 0.9757, Accuracy = 0.8958\n",
      "Epoch 12: Val Loss = 0.3566, AUC = 0.9750, Accuracy = 0.8958\n",
      "Epoch 13: Val Loss = 0.3438, AUC = 0.9758, Accuracy = 0.8996\n",
      "Epoch 14: Val Loss = 0.3335, AUC = 0.9762, Accuracy = 0.9073\n",
      "Epoch 15: Val Loss = 0.3252, AUC = 0.9764, Accuracy = 0.9073\n",
      "Epoch 16: Val Loss = 0.3154, AUC = 0.9766, Accuracy = 0.9073\n",
      "Epoch 17: Val Loss = 0.3074, AUC = 0.9793, Accuracy = 0.9112\n",
      "Epoch 18: Val Loss = 0.3010, AUC = 0.9773, Accuracy = 0.9112\n",
      "Epoch 19: Val Loss = 0.2932, AUC = 0.9797, Accuracy = 0.9151\n",
      "Epoch 20: Val Loss = 0.2886, AUC = 0.9800, Accuracy = 0.9189\n",
      "Epoch 21: Val Loss = 0.2838, AUC = 0.9800, Accuracy = 0.9151\n",
      "Epoch 22: Val Loss = 0.2772, AUC = 0.9798, Accuracy = 0.9151\n",
      "Epoch 23: Val Loss = 0.2748, AUC = 0.9799, Accuracy = 0.9151\n",
      "Epoch 24: Val Loss = 0.2736, AUC = 0.9808, Accuracy = 0.9073\n",
      "Epoch 25: Val Loss = 0.2620, AUC = 0.9807, Accuracy = 0.9189\n",
      "Epoch 26: Val Loss = 0.2605, AUC = 0.9804, Accuracy = 0.9228\n",
      "Epoch 27: Val Loss = 0.2660, AUC = 0.9806, Accuracy = 0.9112\n",
      "Epoch 28: Val Loss = 0.2571, AUC = 0.9803, Accuracy = 0.9112\n",
      "Epoch 29: Val Loss = 0.2547, AUC = 0.9806, Accuracy = 0.9266\n",
      "Epoch 30: Val Loss = 0.2513, AUC = 0.9803, Accuracy = 0.9228\n",
      "\n",
      "=== Cross-Validation Summary ===\n",
      "val_loss: 0.2282 ± 0.0198\n",
      "val_auc: 0.9862 ± 0.0059\n",
      "val_accuracy: 0.9367 ± 0.0187\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# --- Reproducibility ---\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define the path to your CSV file\n",
    "csv_file = 'image_labels.csv'\n",
    "\n",
    "# Initialize empty lists to hold the image paths and labels\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "# Read the CSV file and extract image paths and labels\n",
    "with open(csv_file, mode='r') as file:\n",
    "    reader = csv.DictReader(file)  # Read the CSV into a dictionary\n",
    "    for row in reader:\n",
    "        image_paths.append(row['image_path'])\n",
    "        labels.append(int(row['label']))  # Convert label to integer (0 or 1)\n",
    "\n",
    "# Check the first few entries\n",
    "print(f\"Total images: {len(image_paths)}\")\n",
    "print(f\"First 10 image paths: {image_paths[:10]}\")\n",
    "print(f\"First 10 labels: {labels[:10]}\")\n",
    "\n",
    "# --- Custom Dataset ---\n",
    "class EyeDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return img, label\n",
    "\n",
    "# --- EarlyStopping Class ---\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0, save_path='best_model.pt'):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.save_path = save_path\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        torch.save(model.state_dict(), self.save_path)\n",
    "\n",
    "# --- Transforms ---\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- Cross-validation ---\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "fold_metrics = {\n",
    "    \"val_loss\": [],\n",
    "    \"val_auc\": [],\n",
    "    \"val_accuracy\": []\n",
    "}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(image_paths, labels)):\n",
    "    print(f\"\\n--- Fold {fold + 1} ---\")\n",
    "    train_dataset = EyeDataset([image_paths[i] for i in train_idx],\n",
    "                               [labels[i] for i in train_idx],\n",
    "                               transform=train_transform)\n",
    "    val_dataset = EyeDataset([image_paths[i] for i in val_idx],\n",
    "                             [labels[i] for i in val_idx],\n",
    "                             transform=val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "    model = models.resnet18(pretrained=True) #consider changing\n",
    "    model.fc = nn.Linear(model.fc.in_features, 1)  # no sigmoid for BCEWithLogitsLoss\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Freeze all but final layer\n",
    "    for name, param in model.named_parameters():\n",
    "        if not name.startswith('fc'):\n",
    "            param.requires_grad = False\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "    early_stopping = EarlyStopping(patience=5, save_path=f'best_model_fold{fold+1}.pt')\n",
    "\n",
    "    best_loss, best_auc, best_acc = float('inf'), 0, 0\n",
    "\n",
    "    for epoch in range(30):\n",
    "        model.train()\n",
    "        for imgs, labels_batch in train_loader:\n",
    "            imgs, labels_batch = imgs.to(device), labels_batch.unsqueeze(1).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_losses, preds, truths = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels_batch in val_loader:\n",
    "                imgs, labels_batch = imgs.to(device), labels_batch.unsqueeze(1).to(device)\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, labels_batch)\n",
    "                val_losses.append(loss.item())\n",
    "                preds.extend(torch.sigmoid(outputs).cpu().numpy())\n",
    "                truths.extend(labels_batch.cpu().numpy())\n",
    "\n",
    "        val_loss = np.mean(val_losses)\n",
    "        val_auc = roc_auc_score(truths, preds)\n",
    "        val_acc = accuracy_score(np.array(truths) > 0.5, np.array(preds) > 0.5)\n",
    "        print(f\"Epoch {epoch+1}: Val Loss = {val_loss:.4f}, AUC = {val_auc:.4f}, Accuracy = {val_acc:.4f}\")\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_auc = val_auc\n",
    "            best_acc = val_acc\n",
    "\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    fold_metrics[\"val_loss\"].append(best_loss)\n",
    "    fold_metrics[\"val_auc\"].append(best_auc)\n",
    "    fold_metrics[\"val_accuracy\"].append(best_acc)\n",
    "\n",
    "# --- Results Summary ---\n",
    "print(\"\\n=== Cross-Validation Summary ===\")\n",
    "for key, values in fold_metrics.items():\n",
    "    print(f\"{key}: {np.mean(values):.4f} ± {np.std(values):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save just the weights to model folder\n",
    "torch.save(model.state_dict(), 'resnet18_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
